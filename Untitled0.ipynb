{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOuOjH0AX4f+k8U+3rTvTCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasun/ProgrammingCI/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByE5zUHi4HBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence  =  \"이것이 무엇인가 보고 듣고 먹고 냄새맞고 생각하는 이것이 무엇인가\"\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqVg90s506T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  numpy  as  np  \n",
        "token_sequence  =  str.split(sentence)\n",
        "vocab  =  sorted(set(token_sequence))\n",
        "num_tokens  =  len(token_sequence)\n",
        "vocab_size  =  len(vocab) \n",
        "onehot_vectors  =  np.zeros((num_tokens,vocab_size),  int)\n",
        "for  i,  word  in  enumerate(token_sequence):\n",
        "  onehot_vectors[i,  vocab.index(word)]  =  1\n",
        "\n",
        "print(onehot_vectors)\n",
        "\n",
        "import  pandas  as  pd\n",
        "pd.DataFrame(onehot_vectors,  columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yr0akQZdZtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    #text = text.lower()\n",
        "    #text = text.replace('.', ' .')\n",
        "    #words = text.split(' ')\n",
        "    words = str.split(text)\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''원핫 표현으로 변환\n",
        "\n",
        "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
        "    :param vocab_size: 어휘 수\n",
        "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    '''맥락과 타깃 생성\n",
        "\n",
        "    :param corpus: 말뭉치(단어 ID 목록)\n",
        "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
        "    :return:\n",
        "    '''\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjhM-zEQduLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 1\n",
        "corpus, word_to_id, id_to_word = preprocess(sentence)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "print (target)\n",
        "pd.DataFrame(target,  columns=word_to_id)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}