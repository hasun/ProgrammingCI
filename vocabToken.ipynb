{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vocabToken.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPGU8oI23zXR1Dda3p4+fde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasun/ProgrammingCI/blob/master/vocabToken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByE5zUHi4HBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence  =  \"이것이 무엇인가 보고 듣고 먹고 냄새맞고 생각하는 이것이 무엇인가\"\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqVg90s506T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  numpy  as  np  \n",
        "token_sequence  =  str.split(sentence)\n",
        "vocab  =  sorted(set(token_sequence))\n",
        "num_tokens  =  len(token_sequence)\n",
        "vocab_size  =  len(vocab) \n",
        "onehot_vectors  =  np.zeros((num_tokens,vocab_size),  int)\n",
        "for  i,  word  in  enumerate(token_sequence):\n",
        "  onehot_vectors[i,  vocab.index(word)]  =  1\n",
        "\n",
        "print(onehot_vectors)\n",
        "\n",
        "import  pandas  as  pd\n",
        "pd.DataFrame(onehot_vectors,  columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yr0akQZdZtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''원핫 표현으로 변환\n",
        "\n",
        "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
        "    :param vocab_size: 어휘 수\n",
        "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    '''맥락과 타깃 생성\n",
        "\n",
        "    :param corpus: 말뭉치(단어 ID 목록)\n",
        "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
        "    :return:\n",
        "    '''\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjhM-zEQduLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 1\n",
        "corpus, word_to_id, id_to_word = preprocess(sentence)\n",
        "\n",
        "print (corpus)\n",
        "print (\"키워드 맵핑=======\")\n",
        "print (word_to_id)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "#print (\"타겟 키워드=======\")\n",
        "#print (target)\n",
        "#print (\"타켓 키워드 주변 단어=======\")\n",
        "#print (contexts)\n",
        "\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "#print (\"타켓 one hot encoder 변환=======\")\n",
        "print(pd.DataFrame(target,  columns=word_to_id))\n",
        "\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "#print (\"주변단어 one hot encoder 변환=======\")\n",
        "print (contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHWFH6D6v2kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krix8haZwA5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['너 오늘 이뻐 보인다',\n",
        "          '나는 오늘 기분이 더러워',\n",
        "          '끝내주는데, 좋은 일이 있나봐',\n",
        "          '나 좋은 일이 생겼어',\n",
        "          '아 오늘 진짜 짜증나',\n",
        "          '환상적인데, 정말 좋은거 같아']\n",
        "\n",
        "targets =[[1], [0], [1], [1], [0], [1]]\n",
        "\n",
        "test_samples = ['이뻐 보인다',\n",
        "          '안좋아',\n",
        "          '좋은 일이 있나봐',\n",
        "          '일이 생겼어',\n",
        "          '짜증나',\n",
        "          '좋은거 같아']\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "input_sequences = np.array(sequences)\n",
        "labels = np.array(targets)\n",
        "\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGfHUW9G5VTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYLtCRsw5fXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=5000) \n",
        "\n",
        "X = vectorizer.fit_transform(samples)\n",
        "y = np.array(targets)\n",
        "\n",
        "#print(X)\n",
        "features = vectorizer.get_feature_names()\n",
        "\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "lgs = LogisticRegression(class_weight='balanced') \n",
        "lgs.fit(X_train, y_train) \n",
        "\n",
        "predicted = lgs.predict(X_eval)\n",
        "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))\n",
        "\n",
        "testDataVecs = vectorizer.transform(test_samples)\n",
        "test_predicted = lgs.predict(testDataVecs)\n",
        "print(test_predicted)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcmgC892A1yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(words, model, num_features):\n",
        "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
        "\n",
        "    num_words = 0\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "\n",
        "    for w in words:\n",
        "        if w in index2word_set:\n",
        "            num_words += 1\n",
        "            feature_vector = np.add(feature_vector, model[w])\n",
        "\n",
        "    feature_vector = np.divide(feature_vector, num_words)\n",
        "    return feature_vector\n",
        "\n",
        "def get_dataset(reviews, model, num_features):\n",
        "    dataset = list()\n",
        "\n",
        "    for s in reviews:\n",
        "        dataset.append(get_features(s, model, num_features))\n",
        "\n",
        "    reviewFeatureVecs = np.stack(dataset)\n",
        "    \n",
        "    return reviewFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc81YNBV8oRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 5    \n",
        "min_word_count = 1   \n",
        "num_workers = 4       \n",
        "context = 5          \n",
        "downsampling = 1e-3 \n",
        "\n",
        "from gensim.models import word2vec\n",
        "\n",
        "sentences = []\n",
        "for sample in samples:\n",
        "    sentences.append(sample.split())\n",
        "\n",
        "print (sentences)\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
        "           size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling)\n",
        "\n",
        "test_data_vecs = get_dataset(sentences, model, num_features)\n",
        "\n",
        "X = test_data_vecs\n",
        "y = np.array(targets)\n",
        "\n",
        "print (X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "lgs = LogisticRegression(class_weight='balanced')\n",
        "lgs.fit(X_train, y_train)\n",
        "\n",
        "predicted = lgs.predict(X_test)\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, (lgs.predict_proba(X_test)[:, 1]))\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(\"------------\")\n",
        "print(\"Accuracy: %f\" % lgs.score(X_test, y_test))  #checking the accuracy\n",
        "print(\"Precision: %f\" % metrics.precision_score(y_test, predicted))\n",
        "print(\"Recall: %f\" % metrics.recall_score(y_test, predicted))\n",
        "print(\"F1-Score: %f\" % metrics.f1_score(y_test, predicted))\n",
        "print(\"AUC: %f\" % auc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}